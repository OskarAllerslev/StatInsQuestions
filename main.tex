\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amssymb}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}


\title{OpgavelÃ¸sninger StatIns}
\author{Oskar Allerslev}
\date{August 2025}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Week 1 - sections 1.1  1.2  1.3 }
\begin{itemize}
    \item Exercise 1
    \item Exercise 3
    \item Exercise 4
    \item Practical exercise 1
    \item Practical exercise 2
\end{itemize}

\subsubsection{Exercise 1}
Let $(Z_n)$ and $(W_n)$ be $R^p$-valued r.v. \\
show that if $||Z_n - W_n|| = o_P(1)$ and $Z_n \overset{d}{\rightarrow} Z$  then also $W_n \overset{d}{\rightarrow} Z$

\begin{proof} 
We want to show that 
\begin{align*}
    \lim_{n \rightarrow} E f(W_n) = E f(W)
\end{align*}
We apply the triangle inequality
\begin{align*}
   \lim_{n\rightarrow \infty} E(f(W_n) - f(Z_n)) &\leq \lim_{n \rightarrow \infty} |E(f(W_n)) - E(f(Z_n))| + |E(f(Z_n)  - E(f(Z))|  \\
   &= \lim_{n \rightarrow \infty}|E(f(W_n)) - E(f(Z_n))| + \lim_{n \rightarrow \infty}|E(f(Z_n)  - E(f(Z))|\\
   &\overset{\dagger}{=}\lim_{n \rightarrow \infty}|E(f(W_n)) - E(f(Z_n))|\\ 
   &=\lim_{n \rightarrow \infty}|E(f(W_n)- f(Z_n))|\\ 
   &= \lim_{n\rightarrow \infty} E(| f(W_n) - f(Z_n)|1_{\{ || W_n - Z_n || \leq \delta \}} ) + E(|f(W_n) - f(Z_n)| 1_{\{ || W_n - Z_n || \leq \delta \}^c})\\
   &\leq \lim_{n\rightarrow \infty} L \delta \underbrace{P(|| W_n - Z_n || \leq \delta )}_{ c\geq 0 }+ 2M \underbrace{P(|| W_n - Z_n || > \delta )}_{o_P(1)}\\
   &= 0
   \\
\end{align*}
Where definition 1.2 (convergence in distribution) has been used at $\dagger$. And the definition of Lipschitz continuity at *.
\end{proof}

\subsubsection{Exercise 3}
Show that $\sqrt{n}(\mathbb{F}^{(n)}(t_1) - F(t_1), \dots, \mathbb{F}^{(n)}(t_M) - F(t_M)) \overset{d}{\rightarrow} \mathcal{N}(0, \Sigma) $
\begin{proof}
 We apply theorem 1.4 Laplace multivariate CLT:\\
 We note that accordingo the notation in the theorem we set\\
 \begin{align*}
     Z_k &= (1_{(X_k \leq t_1)},\dots , 1_{(X_k \leq t_M)}) \in \mathbb{R}^M
 \end{align*}
 Which fulfills the condition since $||Z_i||^2 \leq M \Rightarrow E||Z_i||^2 \leq M < \infty$.
 \\
 which gives the desired convergence result. We then look at calculating the covariance matrix. 
 We look at some entrance in the covacovariancerix:
 \begin{align*}
     \text{Cov}(X_i, X_j) &= E((1_{(X_i \leq t_i)} - F(t_i))(1_{(X_j \leq t_j)} - F(t_j))) \\ 
     &= E(1_{(X_i \leq t_i)}1_{(X_j \leq t_j)} + F(t_i)F(t_j) - F(t_i)1_{(X_j \leq t_j)} -F(t_j)1_{(X_i \leq t_i)})\\
     &= F(t_i \land t_j) - F(t_j)F(t_i)
 \end{align*}
\end{proof}


\subsubsection{Exercise 4}
\textbf{a)}
$Z$ is Pareto distributed with scale $\kappa > 0$ and shape $\alpha > 0 $ then $\bar{F}_Z(z) = (\kappa / z )^{\alpha}, z \geq \kappa$.
Show that if $\alpha > 1 \Rightarrow e_F(u) = u / (\alpha -1), u \geq \kappa$ where $e_F(u) = E(Z-u \mid Z  > u)$. 
\begin{proof}
\begin{align*}
    E(Z-u \mid Z > u) &= \frac{E((Z-u)1_{\{ Z > u \}})}{P(Z > u)}\\
\end{align*}
We look at the top integral
\begin{align*}
    \int_{Z > u} Z dP_z - u \int_{Z>u}dP_Z &= \int_u^\infty x \alpha \kappa^\alpha x^{-\alpha} dx - uP(Z > u) \\
    &= \alpha \kappa^\alpha \int_u^\infty x^{-\alpha} dx -u P(Z>u)\\
    &= \alpha \kappa^\alpha u^{-(\alpha -1)}(\alpha -1)^{-1} - u P(Z>u) 
\end{align*}
And we divide through with the bottom
\begin{align*}
   \frac{\alpha \kappa^\alpha u^{-(\alpha -1)}(\alpha -1)^{-1} - u \kappa^\alpha  u^{-\alpha}}{\kappa^\alpha  u^{-\alpha}} 
   &= \frac{\alpha}{\alpha - 1}u - u \\
   &= \frac{u}{\alpha - 1} \hspace{1cm} \forall \alpha > 1
\end{align*}
\end{proof}
\textbf{b)}
If $Z$ is log-normal with $\mu \in R$ and $\sigma > 0$ then the distribution of $Z$ is 
\begin{align*}
    G_Z(z) = \Phi\left( \frac{\log z - \mu}{\sigma} \right), \, z > 0
\end{align*}
its given that 
\begin{align*}
    e_G(u) = \exp{\mu + \frac{\sigma^2}{2}} \frac{\Phi\left( \frac{(\mu + \sigma^2) - \log u}{\sigma} \right)}{\Phi\left( \frac{\mu - \log u}{\sigma}\right)} - u 
\end{align*}
Show that as $u$ increases, 
\begin{align*}
    e_G(u) = \frac{\sigma^2 u }{\log u - (\mu + \sigma^2)} (1+ o(1))
\end{align*}
\begin{proof}

\begin{align*}
    e_G(u) &= e^{\mu + \sigma^2/2} \frac{\Phi(a + \sigma)}{\Phi(a)}-u \\
    &\overset{\text{Mill}}{=} e^{\mu + \sigma^2/2}\frac{\Phi(a + \sigma)}{\Phi(a)} \frac{a}{a+\sigma} -u\\
    &=e^{\mu + \sigma^2/2} e^{-\frac{1}{2}((a + \sigma)^2 - a^2)} \frac{a}{a + \sigma} - u\\
    &= e^{\mu + \sigma^2/2} e^{- a\sigma - \frac{1}{2}\sigma^2} \frac{a}{a + \sigma} - u\\
    &= e^{\mu - a\sigma}\frac{a}{a+\sigma} - u\\
    &= e^{\mu + \log u - \mu}\frac{a}{a+\sigma}- u \\
    &= u \frac{a}{a+\sigma} - u \\
    &= u\left( \frac{a}{a + \sigma}-1\right) \\
    &= -u\frac{a}{a + \sigma} \\
    &=-u  \sigma \left( \frac{\mu + \sigma^2 - \log u}{\sigma} \right)^{-1} \\
    &\overset{Mill}{=} \frac{u\sigma^2}{\log u - (\mu +\sigma^2)}(1 + o(1))
\end{align*}


\end{proof}

\subsubsection{Exercise 10}
\textbf{a)}
\begin{align*}
    \phi(h)(t) &= - \log(1-h(t)) ,\ h(t) < 1
\end{align*}
Find the Hadamard derivative of $\phi$ at a function $f$ in the direction $h$.
\begin{proof}
\begin{align*}
    \frac{\phi(\theta + t_n h_n) - \phi(\theta)}{t_n} &= \frac{-\log(1-(f+t_n h_n)(t)) + \log (1-f(t))}{t_n} \\
    &= -\log\left( \frac{1-f(t) - t_n h_n(t)}{1-f(t)} \right)t_n^{-1}\\
    &= -t_n^{-1} \log\left( 1 - \frac{t_n h_n(t)}{1-f(t)}\right) \\
    &= -t_n^{-1} \frac{-t_n h_n(t)}{1-f(t)} \\
    &= \frac{h_n(t)}{1-f(t)} \\
    &\overset{n\rightarrow \infty}{\rightarrow} \frac{h(t)}{1-f(t)}
\end{align*}
\end{proof}
\textbf{b)}
State and prove a functional limit theorem of $\sqrt{n}(\Delta^{(n)} - \Delta)$ and compute the covariance function.
\begin{proof}

We want to aaply Theorem 1.26 Functional $\delta$-method. 
We note for $\theta: [0,1] \subseteq \mathbb{R}_+ \rightarrow \mathbb{E}$ where $\theta = F$ where $F$ denotes the theoretical cdf. Next let $X_n = \frac{1}{n} \sum_{i=1}^n 1_{(Z_i \leq t)}: \Omega \rightarrow [0,1]$.
First we find:
\begin{align*}
    \Delta^{(n)}(t) &= \log n - \log \left( \sum_{i=1}^n 1_{(Z_i > t)}\right) \\
    &= - \left( - \log n - \log \left( \sum_{i=1}^n 1_{(Z_i > t)}\right) \right)  \\
    &= - \log\left( \frac{1}{n} \sum_{i=1}^n 1_{(Z_i > t)}\right) \\
    &= -\log \left( 1 - \frac{1}{n} \sum_{i=1}^n 1_{(Z_i \leq t)} \right)
\end{align*}
Then we need to show that $r_n(X_n - \theta) \overset{d}{\rightarrow} X$.
We are looking at this functional convergence in terms of convergence in functions and not in terms of pointwise convergence. 
\\ \\
We then apply Theorem $1.30$ which states $\sqrt{n}(\mathbb{F}^{(n)} - F)\overset{d}{\rightarrow} B \circ F$
Which we then apply to the Hadamard derivative we obtained in the previous question:
\begin{align*}
    \sqrt{n}(\phi(X_n) - \phi(\theta)) &\overset{d}{\rightarrow} \phi_\theta^{'}(X) \\
    &= \frac{B \circ F}{1-F}
\end{align*}
Which is then the final result. We are also asked to compute the covariance which is stated for the Brownian Bridge in a footnote.
\begin{align*}
    \text{Cov}\left( \frac{B(F(t)}{1-F(t)},  \frac{B(F(s)}{1-F(s)}\right) &= \frac{F(s\land t) - F(s)F(t)}{(1-F(s))(1-F(t))}
\end{align*}
    
\end{proof}





















\end{document}
